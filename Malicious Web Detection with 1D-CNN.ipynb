{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Description of Malicious Website\nA malicious website is a site that attempts to install malware (a term for anything that will disrupt computer operation, gather your personal information or, in a worst-case scenario, gain total access to your machine) onto your device. This usually requires some action on your part, however, in the case of a drive-by download, the website will attempt to install software on your computer without asking for permission first. (source: https://us.norton.com/internetsecurity-malware-what-are-malicious-websites.html)\n## Instruction\nHere a model will be created to detect malicious websites. Website url is used as a feature and 1D Convolutional Neural Network (CNN) is used as an algorithm for detection malicious websites. Model will be validated by holdout validation\nConsult: https://blog.csdn.net/m0_37876745/article/details/84937339\n## Consult\nhttps://blog.csdn.net/sinat_26917383/article/details/72857454  \nhttps://blog.csdn.net/zwqjoy/article/details/86677030  \nhttps://blog.csdn.net/vesper305/article/details/44927047  \nhttps://blog.csdn.net/akadiao/article/details/78788864  \nhttps://blog.csdn.net/qq_40549291/article/details/85274581  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Setup Notebook","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# tldextract model is used to extract top-level domain from URL\n# Consult: https://blog.csdn.net/weixin_44285988/article/details/89235814\n!pip install tldextract","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Description\nnumpy: Array and matrix operation.  \npandas: Data analysis.  \nre: Match & processing strings using regular expressions.  \nmatplotlib.pyplot: Data visualization.  \nmatplotlib.image: Used fro basic image loading, rescaling and display operations.  \nseaborn: Data visualization  \nrandom: Generate a random number.  \nos: Miscellaneous operating system interfaces.  \npickle:  Python object serialization(Consult: https://docs.python.org/3/library/pickle.html)  \nurllib.parse: Process url\nurlparse: Identification and segmentation of URL.  \ntldextract: vide supra\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport gc\nimport random\nimport os\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.python.util import deprecation\nfrom urllib.parse import urlparse\nimport tldextract","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train_test_split: Split test set training set.  \nTokenizer: Vectorize the text, converting the text to a sequence.  \npad_sequences: https://blog.csdn.net/wcy23580/article/details/84957471  \nbackend: Backend operation.  \nmetrics: Measure loss or change in model accuracy. Consult: https://www.cnblogs.com/zdm-code/p/12244043.html  \nEarlyStopping: Used to stop training ahead of time.  \nplot_model: Depict the neural network by flow chart.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import models, layers, backend, metrics\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set random seed\n# (为了在同样的数据集上获得可复现的训练结果)\n# Consult: https://www.jianshu.com/p/917962bef4a2\nos.environ['PYTHONHASHSEED'] = '0'\n# (log信息输出设置)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nnp.random.seed(0)\nrandom.seed(0)\ntf.set_random_seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# other setup\n# (适应retina屏幕)\n# Consult: \n%config InlineBackend.figure_format = 'retina'\npd.set_option('max_colwidth', 500)\n# Disable deprecation warnings\ndeprecation._PRINT_DEPRECATION_WARNINGS = False","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\ndata = pd.read_csv('../input/data.csv')\n# Shuffle data\ndata = data.sample(frac=1, random_state=0)\nprint(f'Data size: {data.shape}')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook uses hold The holdout method is a method that separates training and test data by 80% and 20%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val_size = 0.2\ntrain_data, val_data = train_test_split(data, test_size=val_size, stratify=data['label'], random_state=0)\nprint(f'Train shape: {train_data.shape}, Validation shape: {val_data.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis and Feature Engineering\nLet's do some data analysis to expand our knowledge of this data and do some feature engineering. First we want to find out whether the data is imbalance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.label.value_counts().plot.barh()\nplt.title('All Data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good, bad = data.label.value_counts()\nprint(f'Ratio of data between target labels (bad & good) is {bad//bad}:{good//bad}')\n# 注意print中{}的用法","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, lets find out the most used suffix domain, domain and sub domain. We need to extract subdomains, domains and domain suffixes to be able to do the analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def parsed_url(url):\n    # extract subdomain, domain, and domain suffix from url\n    # if item == '', fill with '<empty>'(将item == ''的部分用<empty>标签填充)\n    subdomain, domain, domain_suffix = ('<empty>' if extracted == '' else extracted for extracted in tldextract.extract(url))\n    return [subdomain, domain, domain_suffix]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract_url_data = [parsed_url(url) for url in train_data['url']]\n#extract_url_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_url(data):\n    # parsed url\n    extract_url_data = [parsed_url(url) for url in data['url']]\n    extract_url_data = pd.DataFrame(extract_url_data, columns=['subdomain', 'domain', 'domain_suffix'])\n    # concat extracted feature with original data\n    data = data.reset_index(drop=True)\n    data = pd.concat([data, extract_url_data], axis=1)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = extract_url(train_data)\nval_data = extract_url(val_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data.head()\n#val_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot(train_data, val_data, column):\n    plt.figure(figsize=(10, 17))\n    plt.subplot(411)\n    plt.title(f'Train data {column}')\n    plt.ylabel(column)\n    train_data[column].value_counts().head(10).plot.barh()\n    plt.subplot(412)\n    plt.title(f'Validation data {column}')\n    plt.ylabel(column)\n    val_data[column].value_counts().head(10).plot.barh()\n    plt.subplot(413)\n    plt.title(f'Train data {column} (groupped)')\n    plt.ylabel(f'(label, {column})')\n    train_data.groupby('label')[column].value_counts().head(10).plot.barh()\n    plt.subplot(414)\n    plt.title(f'Validation data {column} (groupped)')\n    plt.ylabel(f'(label, {column})')\n    val_data.groupby('label')[column].value_counts().head(10).plot.barh()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train_data, val_data, 'subdomain')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train_data, val_data, 'domain')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train_data, val_data, 'domain_suffix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the plot above there are interesting things to note, there are websites that have google and twitter domains with bad labels. It's time we do the filter to see data with google domains and Twitter with bad labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[(train_data['domain'] == 'google') & (train_data['label'] == 'bad')].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[(train_data['domain'] == 'twitter') & (train_data['label'] == 'bad')].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we need to do tokenization on the url so that it can be used as input to the CNN model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(filters='', char_level=True, lower=False, oov_token=1)\n# fit only on training data\ntokenizer.fit_on_texts(train_data['url'])\ntokenizer.word_index.keys()\n# (token字典键的数量)\nn_char = len(tokenizer.word_index.keys())\nprint(f'N Char: {n_char}')\n# Consult: https://www.cnblogs.com/jielongAI/p/10178585.html\n# Consult: https://www.runoob.com/python/att-dictionary-keys.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq = tokenizer.texts_to_sequences(train_data['url'])\nval_seq = tokenizer.texts_to_sequences(val_data['url'])\nprint('Before tokenization: ')\nprint(train_data.iloc[0]['url'])\nprint('\\nAfter tokenization: ')\nprint(train_seq[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_length = np.array([len(i) for i in train_seq])\nsequence_length = np.percentile(sequence_length, 99).astype(int)\nprint(f'Sequence length: {sequence_length}')\n# Consult: https://blog.csdn.net/ximibbb/article/details/79149887(文本预处理方法)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each text length has a different length, therefore using padding to equalize each text length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq = pad_sequences(train_seq, padding='post', maxlen=sequence_length)\nval_seq = pad_sequences(val_seq, padding='post', maxlen=sequence_length)\nprint('After padding: ')\nprint(train_seq[0])\n# Consult: https://blog.csdn.net/wcy23580/article/details/84957471","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save the tokenizer for later use","execution_count":null},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"with open('tokenizer.pkl', 'wb') as f:\n    pickle.dump(tokenizer, f)\n# Consult: https://blog.csdn.net/gdkyxy2013/article/details/80495353\n# Consult: https://www.cnblogs.com/cainiaoxuexi2017-ZYA/p/11673982.html\n# Consult: https://www.cnblogs.com/zhangbao003/p/8926366.html","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also encode subdomain, domain, suffix domains and label into numerical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_label(label_index, data):\n    try:\n        return label_index[data]\n    except:\n        return label_index['<unknown>']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_value = {}\nfor feature in ['subdomain', 'domain', 'domain_suffix']:\n    # get unique value\n    label_index = {label: index for index, label in enumerate(train_data[feature].unique())}\n    # add unknown label in last index\n    label_index['<unknown>'] = list(label_index.values())[-1] + 1\n    # count unique value\n    unique_value[feature] = label_index['<unknown>']\n    # encode\n    train_data.loc[:, feature] = [encode_label(label_index, i) for i in train_data.loc[:, feature]]\n    val_data.loc[:, feature] = [encode_label(label_index, i) for i in val_data.loc[:, feature]]\n    # save label index\n    with open(f'{feature}.pkl', 'wb') as f:\n        pickle.dump(label_index, f)\n# https://www.runoob.com/python/python-func-enumerate.html\n# https://blog.csdn.net/weixin_39549734/article/details/81224567","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (对标签进行编码)\nfor data in [train_data, val_data]:\n    data.loc[:, 'label'] = [0 if i == 'good' else 1 for i in data.loc[:, 'label']]\n# Consult: https://www.cnblogs.com/zknublx/p/9623080.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Unique subdomain in Train data: {unique_value['subdomain']}\")\nprint(f\"Unique domain in Train data: {unique_value['domain']}\")\nprint(f\"Unique domain suffix in Train data: {unique_value['domain_suffix']}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create CNN Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def convolution_block(x):\n    # 3 sequence conv layer\n    conv_3_layer = layers.Conv1D(64, 3, padding='same', activation='elu')(x)\n    # 5 sequence conv layer\n    conv_5_layer = layers.Conv1D(64, 5, padding='same', activation='elu')(x)\n    # concat conv layer\n    conv_layer = layers.concatenate([x, conv_3_layer, conv_5_layer])\n    # flatten\n    conv_layer = layers.Flatten()(conv_layer)\n    return conv_layer\n# Consult: https://blog.csdn.net/qq_42004289/article/details/105367854\n# Consult: https://blog.csdn.net/kilotwo/article/details/88403079","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def embedding_block(unique_value, size, name):\n    # (构建网络的第一层-输入层)\n    input_layer = layers.Input(shape=(1,), name=name + '_input')\n    # (向量编码转换)\n    embedding_layer = layers.Embedding(unique_value, size, input_length=1)(input_layer)\n    return input_layer, embedding_layer\n# Consult: https://blog.csdn.net/weixin_44441131/article/details/105901178\n# Consult: https://blog.csdn.net/u013249853/article/details/89194787(embedding层)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(sequence_length, n_char, unique_value):\n    input_layer = []\n    # sequence input layer\n    sequence_input_layer = layers.Input(shape=(sequence_length,), name='url_input')\n    input_layer.append(sequence_input_layer)\n    # convolution block\n    char_embedding = layers.Embedding(n_char + 1, 32, input_length=sequence_length)(sequence_input_layer)\n    conv_layer = convolution_block(char_embedding)\n    # entity embedding\n    entity_embedding = []\n    for key, n in unique_value.items():\n        size = 4\n        input_l, embedding_l = embedding_block(n, size, key)\n        embedding_l = layers.Reshape(target_shape=(size,))(embedding_l)\n        input_layer.append(input_l)\n        entity_embedding.append(embedding_l)\n    # concat all layer\n    fc_layer = layers.concatenate([conv_layer, *entity_embedding])\n    fc_layer = layers.Dropout(rate=0.5)(fc_layer)\n    # dense layer\n    fc_layer = layers.Dense(128, activation='elu')(fc_layer)\n    fc_layer = layers.Dropout(rate=0.2)(fc_layer)\n    # output layer\n    output_layer = layers.Dense(1, activation='sigmoid')(fc_layer)\n    model = models.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.Precision(), metrics.Recall()])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reset session\nbackend.clear_session()\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(0)\nrandom.seed(0)\ntf.set_random_seed(0)\n# create model\nmodel = create_model(sequence_length, n_char, unique_value)\nmodel.summary()\n# Consult: https://blog.csdn.net/qq_34418352/article/details/106636200\n# Consult: https://blog.csdn.net/ybdesire/article/details/85217688","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, to_file='model.png')\nmodel_image = mpimg.imread('model.png')\nplt.figure(figsize=(75, 75))\nplt.imshow(model_image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model received 4 inputs, the first input came from URL that has been done tokenization and padding. Other inputs are subdomains, domains and suffix domains that have been encoded. URL input will pass through embedding layer and convolution layer while other input will pass embedding layer. Then the results from each input will be concatenated.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = [train_seq, train_data['subdomain'], train_data['domain'], train_data['domain_suffix']]\ntrain_y = train_data['label']\nval_x = [val_seq, val_data['subdomain'], val_data['domain'], val_data['domain_suffix']]\nval_y = val_data['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = [EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True, mode='max')]\nhistory = model.fit(train_x, train_y, batch_size=64, epochs=25, verbose=1, validation_data=[val_x, val_y], shuffle=True, callbacks=early_stopping)\nmodel.save('model.h5')\n# Consult: https://blog.csdn.net/DoReAGON/article/details/88552892\n# Consult: https://blog.csdn.net/leviopku/article/details/86612293\n# COnsult: https://blog.csdn.net/tszupup/article/details/85198949(可直接加载模型进行训练)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nfor index, key in enumerate(['loss', 'precision', 'recall']):\n    plt.subplot(1, 3, index+1)\n    plt.plot(history.history[key], label=key)\n    plt.plot(history.history[f'val_{key}'], label=f'val {key}')\n    plt.legend()\n    plt.title(f'{key} vs val {key}')\n    plt.ylabel(f'{key}')\n    plt.xlabel('epoch')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val_pred = model.predict(val_x)\nval_pred = np.where(val_pred[:, 0] >= 0.5, 1, 0)\nprint(f'Validation Data:\\n{val_data.label.value_counts()}')\nprint(f'\\n\\nConfusion Matrix:\\n{confusion_matrix(val_y, val_pred)}')\nprint(f'\\n\\nClassification Report:\\n{classification_report(val_y, val_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nIn conclusion, the trained model has a high precision and recall value but what must be considered is the precision value. The precision value must be high because if it is low then a website that is not malicious has the possibility to be classified as malicious","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}